import pandas as pd
import numpy as np
from numpy.linalg import norm
from dotenv import load_dotenv
import os
from openai import OpenAI
import json
import streamlit as st

# Load environment variables
load_dotenv()

# Initialize OpenAI client with API key
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Load the CSV with embeddings
df = pd.read_csv('embeddings.csv')

# Convert string back to numpy arrays in the 'embedding' column
df['embedding'] = df['embedding'].apply(eval).apply(np.array)

# Load JSON data from file
with open('document.json', 'r', encoding='utf-8') as file:
    document_data = json.load(file)

# Convert JSON data into a dictionary for quick access
document_dict = {}
for doc in document_data:
    for section in doc['sections']:
        key = (doc['filename'], section['title'])
        document_dict[key] = section['content']

# Define a threshold for minimum similarity
SIMILARITY_THRESHOLD = 0.3  # Adjust this value based on experimentation

def get_embedding(text, model="text-embedding-3-small"):
    if not isinstance(text, str) or not text.strip():
        st.warning("Invalid or empty text input detected.")
        return np.zeros(1536)  # Return a zero vector for invalid input
    text = text.replace("\n", " ")  # Normalize newlines
    try:
        response = client.embeddings.create(input=text, model=model)
        response_dict = response.to_dict()  # Convert the response object to a dictionary
        embedding_vector = response_dict['data'][0]['embedding']
        return embedding_vector
    except Exception as e:
        st.error(f"An error occurred: {e}")
        return np.zeros(1536)  # Return a zero vector if there's an error

def safe_divide(a, b):
    return a / b if b != 0 else 0

def get_response(user_query):
    user_embedding = get_embedding(user_query, model='text-embedding-3-small')
    user_embedding_np = np.array(user_embedding)

    # Compute cosine similarity between user query and all document embeddings
    similarities = df['embedding'].apply(lambda x: np.dot(x, user_embedding_np) / safe_divide(norm(x) * norm(user_embedding_np), 1))

    # Find the indices of the top 3 most similar documents
    top_indices = similarities.nlargest(3).index

    # Check if the top similarity score meets the threshold
    if similarities.iloc[top_indices[0]] < SIMILARITY_THRESHOLD:
        # If the highest similarity is below the threshold, return a default response
        return "Ich weiß es nicht.", None  # None indicates no references

    # Collect context from the top 3 documents and track references
    context = ""
    references = []  # List to store references
    for index in top_indices:
        filename = df.iloc[index]['filename']
        section_title = df.iloc[index]['section_title']
        content_key = (filename, section_title)
        if content_key in document_dict:
            context += document_dict[content_key] + " \n\n"
            references.append({"filename": filename, "title": section_title})  # Collect reference details

    # Generate response using OpenAI's LLM
    if context.strip():
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "Du bist ein Assistent, der Fragen zu Dokumenten detailliert und professionell anhand des untenstehenden Kontextes beantwortet. Sollte die Frage aufgrund des Kontextes nicht beantwortet werden können, antworte bitte mit \"Ich weiß es nicht.\""},
                    {"role": "user", "content": f"Kontext: {context}\n\nFrage: {user_query}\nAntwort:"}
                ],
                temperature=0,
                max_tokens=300
            )

            if response.choices:
                message_text = response.choices[0].message.content.strip()
                return message_text, references  # Return both the answer and references
            else:
                return "No response generated by the model.", references
        except Exception as e:
            st.error(f"An error occurred during response generation: {e}")
            return "An error occurred while generating the response.", references
    else:
        return "Not enough content to generate a response.", references

# Streamlit UI
st.title("Wissenschaftliches Arbeiten und Kommunizieren")

# User input for the query
user_query = st.text_input("Gib deine Frage ein:")
if user_query:
    response, references = get_response(user_query)  # Get response and references
    st.write("### Antwort:")
    st.write(response)
    
    # Display references or an unrelated message if no references
    if references:
        st.write("### Quellenangaben:")
        for ref in references:
            st.write(f"- **Datei**: {ref['filename']} | **Titel**: {ref['title']}")
    else:
        st.write("### Hinweis:")
        st.write("Die Frage scheint nicht im Zusammenhang mit den Inhalten der Wissensdatenbank zu stehen.")
