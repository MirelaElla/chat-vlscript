import pandas as pd
import numpy as np
from numpy.linalg import norm
from dotenv import load_dotenv
import os
from openai import OpenAI
import json
import streamlit as st

# Load environment variables
load_dotenv()

# Initialize OpenAI client with API key
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Load the CSV with embeddings
df = pd.read_csv('embeddings.csv')

# Convert string back to numpy arrays in the 'embedding' column
df['embedding'] = df['embedding'].apply(eval).apply(np.array)

# Load JSON data from file
with open('document.json', 'r', encoding='utf-8') as file:
    document_data = json.load(file)

# Convert JSON data into a dictionary for quick access
document_dict = {}
for doc in document_data:
    for section in doc['sections']:
        key = (doc['filename'], section['title'])
        document_dict[key] = section['content']

def get_embedding(text, model="text-embedding-3-small"):
    if not isinstance(text, str) or not text.strip():
        st.warning("Invalid or empty text input detected.")
        return np.zeros(1536)  # Return a zero vector for invalid input
    text = text.replace("\n", " ")  # Normalize newlines
    try:
        response = client.embeddings.create(input=text, model=model)
        response_dict = response.to_dict()  # Convert the response object to a dictionary
        embedding_vector = response_dict['data'][0]['embedding']
        return embedding_vector
    except Exception as e:
        st.error(f"An error occurred: {e}")
        return np.zeros(1536)  # Return a zero vector if there's an error

def safe_divide(a, b):
    return a / b if b != 0 else 0

def get_response(user_query):
    user_embedding = get_embedding(user_query, model='text-embedding-3-small')
    user_embedding_np = np.array(user_embedding)

    # Compute cosine similarity between user query and all document embeddings
    similarities = df['embedding'].apply(lambda x: np.dot(x, user_embedding_np) / safe_divide(norm(x) * norm(user_embedding_np), 1))

    # Find the indices of the top 3 most similar documents
    top_indices = similarities.nlargest(3).index

    # Collect context from the top 3 documents
    context = ""
    for index in top_indices:
        filename = df.iloc[index]['filename']
        section_title = df.iloc[index]['section_title']
        content_key = (filename, section_title)
        if content_key in document_dict:
            context += document_dict[content_key] + " \n\n"

    # Generate response using OpenAI's LLM
    if context.strip():
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "Du bist ein Assistent, der Fragen zu Dokumenten detailliert und professionell anhand des untenstehenden Kontextes beantwortet. Sollte die Frage aufgrund des Kontextes nicht beantwortet werden k√∂nnen, antworte bitte mit \"Ich weiss es nicht.\""},
                    {"role": "user", "content": f"Kontext: {context}\n\nFrage: {user_query}\nAntwort:"}
                ],
                temperature=0,
                max_tokens=300
            )

            if response.choices:
                message_text = response.choices[0].message.content.strip()
                return message_text
            else:
                return "No response generated by the model."
        except Exception as e:
            st.error(f"An error occurred during response generation: {e}")
            return "An error occurred while generating the response."
    else:
        return "Not enough content to generate a response."

# Streamlit UI
st.title("Wissenschaftliches Arbeiten und Kommunizieren")

# User input for the query
user_query = st.text_input("Gib deine Frage ein:")
if user_query:
    response = get_response(user_query)
    st.write("### Antwort:")
    st.write(response)
