import pandas as pd
import numpy as np
from numpy.linalg import norm
from dotenv import load_dotenv
import os
from openai import OpenAI
import json

# Load environment variables
load_dotenv()


# Initialize OpenAI client with API key
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Load the CSV with embeddings
df = pd.read_csv('embeddings.csv')

# Convert string back to numpy arrays in the 'embedding' column
df['embedding'] = df['embedding'].apply(eval).apply(np.array)

# Load JSON data from file
with open('document.json', 'r', encoding='utf-8') as file:
    document_data = json.load(file)

# Convert JSON data into a dictionary for quick access
document_dict = {}
for doc in document_data:
    for section in doc['sections']:
        key = (doc['filename'], section['title'])
        document_dict[key] = section['content']

def get_embedding(text, model="text-embedding-3-small"):
    if not isinstance(text, str) or not text.strip():
        print("Invalid or empty text input detected, returning zero vector.")
        return np.zeros(1536)  # Return a zero vector for invalid input
    text = text.replace("\n", " ")  # Normalize newlines
    try:
        response = client.embeddings.create(input=text, model=model)
        response_dict = response.to_dict()  # Convert the response object to a dictionary
        embedding_vector = response_dict['data'][0]['embedding']
        return embedding_vector
    except Exception as e:
        print(f"An error occurred: {e}")
        return np.zeros(1536)  # Return a zero vector if there's an error

def safe_divide(a, b):
    if b == 0:
        return 0
    else:
        return a / b    

def chat_with_document_base():
    user_query = input("Frage eingeben: ")
    if user_query.strip():  # Only proceed if the input is not empty
        user_embedding = get_embedding(user_query, model='text-embedding-3-small')
        
        # Convert the user_embedding to a numpy array for comparison
        user_embedding_np = np.array(user_embedding)


        # Compute cosine similarity between user query and all document embeddings
        similarities = df['embedding'].apply(lambda x: np.dot(x, user_embedding_np) / safe_divide(norm(x) * norm(user_embedding_np), 1))
        #print to investigate RuntimeWarning: invalid value encountered in scalar divide
        #print(similarities)
        #print divisor 

        # Find the indices of the top 3 most similar documents
        top_indices = similarities.nlargest(3).index

        # Collect context from the top 3 documents
        context = ""
        for index in top_indices:
            filename = df.iloc[index]['filename']
            section_title = df.iloc[index]['section_title']
            content_key = (filename, section_title)
            if content_key in document_dict:
                context += document_dict[content_key] + " \n\n"  # Add a space between contents

        print("Context for response:")
        print(context)
        # Generate response using OpenAI's LLM
        if context.strip():  # Ensure context is not empty
            try:
                # Correctly parsing the message from the response
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "Du bist ein Assistent, der Fragen zu Dokumenten detailliert und professionell anhand des untenstehenden Kontextes beantwortet. Sollte die Frage aufgrund des Kontextes nicht beantwortet werden k√∂nnen, antworte bitte mit \"Ich weiss es nicht.\"\n\n"},
                        {"role": "user", "content": f"Kontext: {context}\n\n---\n\nFrage: {user_query}\nAntwort:"}
                    ],
                    temperature=0, # Set temperature to 0 to get deterministic results
                    max_tokens=300,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                    seed=0, # Set seed for reproducibility
                )

                # Accessing the text in the correct way:
                if response.choices:
                    #print ("Response generated by the model:")
                    #print(response)
                    message_text = response.choices[0].message.content.strip()  # Corrected access
                    print("AI Generated Response:")
                    print(message_text)
                else:
                    print("No response generated by the model.")
            except Exception as e:
                print(f"An error occurred during response generation: {e}")
        else:
            print("Not enough content to generate a response.")
    else:
        print("No input provided. Please enter a valid query.")

while True:
    chat_with_document_base()
    if input("Continue chatting? (yes/no): ").strip().lower() != 'yes':
        break
